{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOolkAQT5pom"
      },
      "outputs": [],
      "source": [
        "!pip install sentencepiece\n",
        "!pip install transformers==4.28.0\n",
        "!pip install datasets evaluate\n",
        "!pip install sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, M2M100ForConditionalGeneration,Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq,pipeline\n",
        "from huggingface_hub import notebook_login\n",
        "from datasets import load_dataset,load_metric\n",
        "import evaluate\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "m-leExR55yAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The model we will use for training Sanskrit-English model is “facebook/m2m100_418M” which is Meta’s open source model with 418 Million parameters.\n",
        "\n",
        "model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")"
      ],
      "metadata": {
        "id": "8ilBf5C556Dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading Dataset\n",
        "#Itihasa is a Sanskrit-English translation corpus containing 93,000 Sanskrit shlokas and their English translations extracted from M.N.Dutt’s seminal works on The Ramayana and The Mahabharata.\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"rahular/itihasa\")\n",
        "dataset\n"
      ],
      "metadata": {
        "id": "hzow178S56H5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization\n",
        "'''\n",
        "The tokenize_function is specifically designed to process a batch of translation data. Each batch consists of pairs of Sanskrit and English texts extracted from the dataset. The function starts by separating the Sanskrit and English texts from each translation pair in the batch. This separation is achieved using list comprehensions that iterate through the 'translation' field of each entry in the batch, resulting in two separate lists: one for the Sanskrit texts (sanskrit_texts) and another for their corresponding English translations (english_texts).\n",
        "\n",
        "Next, the function tokenizes these texts using a pre-defined tokenizer, which is applied separately to the Sanskrit texts (inputs) and the English texts (targets). The tokenizer is configured with the following parameters: it truncates texts longer than a maximum length to handle variability in text length, pads sequences to a uniform length of 128 tokens, and outputs the tokenized data as PyTorch tensors ('pt').\n",
        "\n",
        "After tokenization, the function organizes the tokens into a structured format suitable for training a sequence-to-sequence (Seq2Seq) model. It returns a dictionary containing the following elements:\n",
        "\n",
        "input_ids: The token IDs representing the Sanskrit texts.\n",
        "\n",
        "attention_mask: The attention masks for the Sanskrit texts, indicating which tokens should be attended to by the model.\n",
        "\n",
        "decoder_input_ids: The token IDs for the English texts, which serve as inputs to the decoder of the Seq2Seq model.\n",
        "\n",
        "decoder_attention_mask: The attention masks for the English texts, guiding the decoder on which tokens to focus.\n",
        "\n",
        "labels: The labels used for training the Seq2Seq model. These are identical to the decoder_input_ids but are cloned to serve as the target outputs during the training process.\n",
        "'''"
      ],
      "metadata": {
        "id": "Fw02eIe256Pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(batch):\n",
        "    sanskrit_texts = [entry['sn'] for entry in batch['translation']]\n",
        "    english_texts = [entry['en'] for entry in batch['translation']]\n",
        "\n",
        "    # Tokenize the inputs (Sanskrit) and targets (English)\n",
        "    inputs = tokenizer(sanskrit_texts, truncation=True, padding='max_length', max_length=128, return_tensors=\"pt\")\n",
        "    targets = tokenizer(english_texts, truncation=True, padding='max_length', max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "    # Return both inputs and targets tokens\n",
        "    return {\n",
        "        \"input_ids\": inputs[\"input_ids\"],\n",
        "        \"attention_mask\": inputs[\"attention_mask\"],\n",
        "        \"decoder_input_ids\": targets[\"input_ids\"],\n",
        "        \"decoder_attention_mask\": targets[\"attention_mask\"],\n",
        "        \"labels\": targets[\"input_ids\"].clone()  # labels for Seq2Seq models are typically the target input_ids\n",
        "    }\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "WRgZ4pza56UK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Transformers Library: The DataCollatorForSeq2Seq is a critical component from the Transformers library, which is widely used for advanced machine learning models.\n",
        "\n",
        "Purpose: The DataCollatorForSeq2Seq efficiently prepares batches of data for sequence-to-sequence (Seq2Seq) models, ensuring data is formatted correctly for training.\n",
        "\n",
        "Initialization: The data collator is initialized by passing two key arguments:\n",
        "\n",
        "Tokenizer: Converts text into a format that the model can understand.\n",
        "Model: The Seq2Seq model that will be trained.\n",
        "Functionality: By using these arguments, the data collator ensures that batches are tokenized and formatted to meet the requirements of the Seq2Seq model.\n",
        "\n",
        "Outcome: This setup helps in the seamless training of Seq2Seq models by handling data preprocessing automatically.\n",
        "'''"
      ],
      "metadata": {
        "id": "jiRbZGsp56YY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
      ],
      "metadata": {
        "id": "VSBxSf8s56eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Importing Evaluation Metrics: We will import two evaluation metrics: SacreBLEU and METEOR.\n",
        "\n",
        "SacreBLEU Metric:\n",
        "Loading: SacreBLEU is loaded using the evaluate.load method.\n",
        "Purpose: SacreBLEU is a standard metric for evaluating machine translation quality.\n",
        "Functionality: It compares machine-generated translations with one or more reference translations, providing a score that reflects the translation's accuracy and fluency.\n",
        "\n",
        "METEOR Metric:\n",
        "Loading: METEOR is loaded using the load_metric function.\n",
        "Purpose: METEOR is another popular metric for evaluating machine translation.\n",
        "Differences from BLEU: Unlike BLEU, METEOR considers factors like synonyms and stemming, providing a more nuanced assessment of translation quality.\n",
        "'''"
      ],
      "metadata": {
        "id": "XU8mPESY7Rj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "from datasets import load_metric\n",
        "\n",
        "metric = evaluate.load(\"sacrebleu\")\n",
        "meteor = load_metric('meteor')"
      ],
      "metadata": {
        "id": "NHiHnc617RpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Postprocessing Functions\n",
        "Overview: Two functions, postprocess_text and compute_metrics, are defined to evaluate the performance of a machine translation model.\n",
        "\n",
        "postprocess_text(preds, labels):\n",
        "Purpose: Cleans and prepares predictions and labels for evaluation.\n",
        "Functionality:\n",
        "Strips leading and trailing spaces from the model's predictions (preds) and ground truth labels (labels).\n",
        "Converts the labels into a list of lists, where each inner list contains a single label.\n",
        "\n",
        "compute_metrics(eval_preds):\n",
        "Purpose: Computes evaluation metrics for the machine translation model.\n",
        "Functionality:\n",
        "Separates the raw predictions and labels from the model evaluation.\n",
        "If predictions are in a tuple format, extracts the necessary part.\n",
        "Decodes predictions and labels from their tokenized form back into text, removing special tokens (like padding or start/end tokens).\n",
        "Applies the postprocess_text function to clean and format the decoded predictions and labels.\n",
        "Computes evaluation metrics such as BLEU and METEOR scores, which are standard for assessing machine translation quality.\n",
        "Calculates the average length of the predictions, indicating the model’s verbosity or succinctness.\n",
        "Rounds the metric scores for easier interpretation and returns the results.\n",
        "'''"
      ],
      "metadata": {
        "id": "BZgJ7UXY7RtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "\n",
        "    return preds, labels\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    meteor_result = meteor.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    result = {'bleu' : result['score']}\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    result[\"meteor\"] = meteor_result[\"meteor\"]\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result"
      ],
      "metadata": {
        "id": "G0GNeGfu7RwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Training Configuration\n",
        "\n",
        "Purpose: The code snippet configures training arguments for a sequence-to-sequence (Seq2Seq) model using the Hugging Face Transformers library.\n",
        "Importance: This setup is essential for specifying the parameters and settings that define the model's training process.\n",
        "Components: The configuration includes details such as:\n",
        "Learning rate: Controls how much the model's weights are adjusted during training.\n",
        "Batch size: Determines the number of training samples used in one iteration.\n",
        "Number of epochs: Specifies how many times the model will pass through the entire training dataset.\n",
        "Save and evaluation frequency: Defines when the model's performance will be evaluated and when checkpoints will be saved during training.\n",
        "Outcome: Proper configuration ensures the Seq2Seq model is trained effectively and efficiently, maximizing performance and minimizing errors.\n",
        "'''"
      ],
      "metadata": {
        "id": "HXPwDerr8BYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"M2M101\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=5,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# for starting the training of model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "HbS6YSZS8Bd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, in order to push the model to huggingface hub so that we can reuse later we have to login into huggingface cli.\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "_4dyiRkA8VCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "zR2UCm-o8VGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Testing\n",
        "\n",
        "text = \"सत्यमेवेश्वरो लोके सत्यं पद्माश्रिता सदा\"\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "translator = pipeline(\"translation\", model=\"my_sanskrit_model\")\n",
        "translator(text)"
      ],
      "metadata": {
        "id": "8DsjZswd8Bhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xYyoEIhe8pIf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}